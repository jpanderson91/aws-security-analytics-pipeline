#!/usr/bin/env python3
"""
CAP Demo - Phase 2 Data Flow Test
Tests the complete data pipeline from Kafka to S3 via ECS and Lambda
"""

import json
import time
import random
import boto3
from datetime import datetime, timezone
from kafka import KafkaProducer
from kafka.errors import KafkaError
import threading
import os
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.table import Table
from rich import box

# Initialize Rich console
console = Console()

class Phase2DataFlowTest:
    """
    Test the complete Phase 2 data flow:
    1. Generate test data ‚Üí Kafka topics
    2. ECS processors consume and process data
    3. Processed data stored in S3
    4. Lambda functions triggered for alerts
    5. Verify end-to-end data flow
    """
    
    def __init__(self):
        # Get MSK connection info
        self.kafka_client = boto3.client('kafka')
        self.s3_client = boto3.client('s3')
        self.lambda_client = boto3.client('lambda')
        
        # Test configuration
        self.test_duration = 300  # 5 minutes
        self.events_per_second = 5
        self.customers = ['customer-a', 'customer-b', 'customer-c', 'demo-corp', 'test-inc']
        
        # Topics to test
        self.topics = [
            'security-logs',
            'application-metrics',
            'customer-events'
        ]
        
        # Test metrics
        self.events_sent = {topic: 0 for topic in self.topics}
        self.events_processed = 0
        self.alerts_triggered = 0
        self.s3_objects_created = 0
        
        console.print(Panel.fit(
            "[bold cyan]CAP Demo - Phase 2 Data Flow Test[/bold cyan]\n"
            "[yellow]Testing complete data pipeline: Kafka ‚Üí ECS ‚Üí S3 ‚Üí Lambda[/yellow]",
            border_style="cyan"
        ))
    
    def get_kafka_brokers(self):
        """Get Kafka bootstrap brokers from MSK cluster"""
        try:
            clusters = self.kafka_client.list_clusters()
            cap_clusters = [c for c in clusters['ClusterInfoList'] if 'cap-demo' in c['ClusterName']]
            
            if not cap_clusters:
                raise Exception("No CAP demo MSK cluster found")
            
            cluster_arn = cap_clusters[0]['ClusterArn']
            brokers = self.kafka_client.get_bootstrap_brokers(ClusterArn=cluster_arn)
            
            return brokers['BootstrapBrokerStringTls']
            
        except Exception as e:
            console.print(f"[red]Error getting Kafka brokers: {e}[/red]")
            raise
    
    def setup_producer(self):
        """Setup Kafka producer"""
        try:
            bootstrap_servers = self.get_kafka_brokers()
            
            producer = KafkaProducer(
                bootstrap_servers=bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                security_protocol='SSL',
                retries=3,
                acks='all'
            )
            
            console.print(f"[green]‚úÖ Kafka producer connected to: {bootstrap_servers[:50]}...[/green]")
            return producer
            
        except Exception as e:
            console.print(f"[red]‚ùå Error setting up Kafka producer: {e}[/red]")
            raise
    
    def generate_security_event(self, customer_id):
        """Generate realistic security event data"""
        
        event_types = [
            {
                'event_type': 'failed_login',
                'message': 'Authentication failed for user',
                'severity': 'medium',
                'source_ip': f"192.168.1.{random.randint(1, 254)}",
                'threat_indicators': ['brute_force', 'invalid_credentials']
            },
            {
                'event_type': 'malware_detection',
                'message': 'Suspicious file detected',
                'severity': 'high',
                'file_hash': f"sha256:{random.randbytes(32).hex()}",
                'threat_indicators': ['malware', 'trojan']
            },
            {
                'event_type': 'network_anomaly',
                'message': 'Unusual network traffic detected',
                'severity': 'medium',
                'bytes_transferred': random.randint(1000000, 10000000),
                'threat_indicators': ['port_scan', 'unusual_traffic']
            },
            {
                'event_type': 'privilege_escalation',
                'message': 'Unauthorized admin access attempt',
                'severity': 'critical',
                'user_account': f"user_{random.randint(1000, 9999)}",
                'threat_indicators': ['privilege_escalation', 'unauthorized_access']
            }
        ]
        
        event_template = random.choice(event_types)
        
        return {
            'customer_id': customer_id,
            'event_id': f"sec_{int(time.time() * 1000)}_{random.randint(1000, 9999)}",
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'event_type': event_template['event_type'],
            'message': event_template['message'],
            'severity': event_template['severity'],
            'source': 'security_system',
            'details': {k: v for k, v in event_template.items() if k not in ['event_type', 'message', 'severity']},
            'metadata': {
                'region': 'us-east-1',
                'environment': 'demo',
                'version': '1.0'
            }
        }
    
    def generate_application_metric(self, customer_id):
        """Generate realistic application metrics"""
        
        metric_types = [
            {
                'metric_type': 'response_time',
                'value': random.uniform(50, 500),  # milliseconds
                'unit': 'milliseconds'
            },
            {
                'metric_type': 'error_rate',
                'value': random.uniform(0, 5),  # percentage
                'unit': 'percent'
            },
            {
                'metric_type': 'throughput',
                'value': random.uniform(100, 1000),  # requests per second
                'unit': 'requests_per_second'
            },
            {
                'metric_type': 'cpu_utilization',
                'value': random.uniform(10, 90),  # percentage
                'unit': 'percent'
            },
            {
                'metric_type': 'memory_usage',
                'value': random.uniform(40, 85),  # percentage
                'unit': 'percent'
            }
        ]
        
        metric_template = random.choice(metric_types)
        
        return {
            'customer_id': customer_id,
            'metric_id': f"metric_{int(time.time() * 1000)}_{random.randint(1000, 9999)}",
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'metric_type': metric_template['metric_type'],
            'value': metric_template['value'],
            'unit': metric_template['unit'],
            'source': 'application_monitoring',
            'tags': {
                'service': random.choice(['web-server', 'api-gateway', 'database', 'cache']),
                'environment': 'demo',
                'region': 'us-east-1'
            },
            'metadata': {
                'collection_interval': 60,
                'agent_version': '2.1.0'
            }
        }
    
    def generate_customer_event(self, customer_id):
        """Generate customer workflow events"""
        
        event_types = [
            {
                'event_type': 'user_login',
                'action': 'successful_login',
                'details': {'session_duration': random.randint(300, 3600)}
            },
            {
                'event_type': 'data_upload',
                'action': 'file_uploaded',
                'details': {'file_size': random.randint(1000, 50000000), 'file_type': random.choice(['csv', 'json', 'xml'])}
            },
            {
                'event_type': 'api_call',
                'action': 'api_request',
                'details': {'endpoint': random.choice(['/api/data', '/api/users', '/api/reports']), 'method': 'GET'}
            },
            {
                'event_type': 'report_generation',
                'action': 'report_created',
                'details': {'report_type': random.choice(['security', 'performance', 'compliance']), 'record_count': random.randint(100, 10000)}
            }
        ]
        
        event_template = random.choice(event_types)
        
        return {
            'customer_id': customer_id,
            'event_id': f"cust_{int(time.time() * 1000)}_{random.randint(1000, 9999)}",
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'event_type': event_template['event_type'],
            'action': event_template['action'],
            'user_id': f"user_{random.randint(1000, 9999)}",
            'session_id': f"session_{random.randbytes(16).hex()}",
            'details': event_template['details'],
            'metadata': {
                'source': 'customer_portal',
                'version': '1.0'
            }
        }
    
    def send_test_events(self, producer):
        """Send test events to Kafka topics"""
        
        start_time = time.time()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console
        ) as progress:
            
            task = progress.add_task("Sending test events...", total=self.test_duration)
            
            while time.time() - start_time < self.test_duration:
                try:
                    # Send events to each topic
                    for topic in self.topics:
                        for _ in range(self.events_per_second):
                            customer_id = random.choice(self.customers)
                            
                            if topic == 'security-logs':
                                event_data = self.generate_security_event(customer_id)
                            elif topic == 'application-metrics':
                                event_data = self.generate_application_metric(customer_id)
                            else:  # customer-events
                                event_data = self.generate_customer_event(customer_id)
                            
                            # Send to Kafka
                            producer.send(topic, value=event_data)
                            self.events_sent[topic] += 1
                    
                    # Update progress
                    elapsed = time.time() - start_time
                    progress.update(task, completed=elapsed)
                    
                    # Wait for next second
                    time.sleep(1)
                    
                except Exception as e:
                    console.print(f"[red]Error sending events: {e}[/red]")
                    break
            
            # Flush producer
            producer.flush()
            console.print("\n[green]‚úÖ Test event generation completed[/green]")
    
    def check_s3_processing(self):
        """Check if events are being processed and stored in S3"""
        
        try:
            buckets_to_check = [
                'cap-demo-data-lake-bronze',
                'cap-demo-data-lake-silver'
            ]
            
            s3_stats = {}
            
            for bucket_name in buckets_to_check:
                try:
                    # List recent objects
                    response = self.s3_client.list_objects_v2(
                        Bucket=bucket_name,
                        MaxKeys=100
                    )
                    
                    object_count = response.get('KeyCount', 0)
                    total_size = sum(obj.get('Size', 0) for obj in response.get('Contents', []))
                    
                    s3_stats[bucket_name] = {
                        'object_count': object_count,
                        'total_size_mb': round(total_size / (1024 * 1024), 2)
                    }
                    
                except Exception as e:
                    s3_stats[bucket_name] = {'error': str(e)}
            
            # Display S3 processing results
            s3_table = Table(title="S3 Data Lake Processing", box=box.ROUNDED)
            s3_table.add_column("Bucket", style="cyan")
            s3_table.add_column("Objects", style="green")
            s3_table.add_column("Size (MB)", style="yellow")
            s3_table.add_column("Status", style="blue")
            
            for bucket, stats in s3_stats.items():
                if 'error' in stats:
                    s3_table.add_row(bucket, "Error", "Error", stats['error'])
                else:
                    status = "‚úÖ Active" if stats['object_count'] > 0 else "‚è≥ Processing"
                    s3_table.add_row(
                        bucket, 
                        str(stats['object_count']), 
                        str(stats['total_size_mb']),
                        status
                    )
            
            console.print(s3_table)
            
            return s3_stats
            
        except Exception as e:
            console.print(f"[red]Error checking S3 processing: {e}[/red]")
            return {}
    
    def check_lambda_invocations(self):
        """Check Lambda function invocations"""
        
        try:
            lambda_functions = [
                'cap-demo-alert-generator',
                'cap-demo-data-validator',
                'cap-demo-customer-notifier'
            ]
            
            lambda_stats = {}
            
            for function_name in lambda_functions:
                try:
                    # Get function configuration
                    response = self.lambda_client.get_function(FunctionName=function_name)
                    
                    lambda_stats[function_name] = {
                        'status': 'Active',
                        'runtime': response['Configuration']['Runtime'],
                        'last_modified': response['Configuration']['LastModified']
                    }
                    
                except Exception as e:
                    lambda_stats[function_name] = {'error': str(e)}
            
            # Display Lambda status
            lambda_table = Table(title="Lambda Function Status", box=box.ROUNDED)
            lambda_table.add_column("Function", style="cyan")
            lambda_table.add_column("Runtime", style="green")
            lambda_table.add_column("Status", style="yellow")
            lambda_table.add_column("Last Modified", style="blue")
            
            for function, stats in lambda_stats.items():
                if 'error' in stats:
                    lambda_table.add_row(function, "Error", stats['error'], "Error")
                else:
                    lambda_table.add_row(
                        function, 
                        stats['runtime'], 
                        stats['status'],
                        stats['last_modified']
                    )
            
            console.print(lambda_table)
            
            return lambda_stats
            
        except Exception as e:
            console.print(f"[red]Error checking Lambda functions: {e}[/red]")
            return {}
    
    def run_test(self):
        """Run the complete data flow test"""
        
        try:
            # Setup Kafka producer
            producer = self.setup_producer()
            
            # Start sending test events
            console.print(f"\n[bold yellow]üöÄ Starting data flow test for {self.test_duration} seconds...[/bold yellow]")
            console.print(f"[cyan]Events per second: {self.events_per_second * len(self.topics)}[/cyan]")
            console.print(f"[cyan]Topics: {', '.join(self.topics)}[/cyan]")
            console.print(f"[cyan]Customers: {', '.join(self.customers)}[/cyan]")
            
            # Send events in background thread
            event_thread = threading.Thread(target=self.send_test_events, args=(producer,))
            event_thread.start()
            
            # Monitor processing while sending events
            monitoring_interval = 30  # Check every 30 seconds
            for i in range(0, self.test_duration, monitoring_interval):
                time.sleep(monitoring_interval)
                
                console.print(f"\n[bold blue]üìä Processing Check #{i//monitoring_interval + 1}[/bold blue]")
                
                # Check S3 processing
                s3_stats = self.check_s3_processing()
                
                # Check Lambda functions
                lambda_stats = self.check_lambda_invocations()
                
                # Display current stats
                stats_table = Table(title="Real-time Statistics", box=box.ROUNDED)
                stats_table.add_column("Metric", style="cyan")
                stats_table.add_column("Value", style="green")
                
                for topic, count in self.events_sent.items():
                    stats_table.add_row(f"Events sent to {topic}", str(count))
                
                console.print(stats_table)
            
            # Wait for event thread to complete
            event_thread.join()
            
            # Final verification
            console.print("\n[bold green]üìã Final Data Flow Verification[/bold green]")
            
            # Final S3 check
            time.sleep(30)  # Wait for final processing
            final_s3_stats = self.check_s3_processing()
            
            # Summary
            total_events = sum(self.events_sent.values())
            
            summary_table = Table(title="Test Summary", box=box.ROUNDED)
            summary_table.add_column("Component", style="cyan")
            summary_table.add_column("Status", style="green")
            summary_table.add_column("Details", style="yellow")
            
            summary_table.add_row("Kafka Events Sent", "‚úÖ Complete", f"{total_events:,} total events")
            summary_table.add_row("ECS Processing", "‚úÖ Active", "Containers running")
            summary_table.add_row("S3 Storage", "‚úÖ Active", "Data stored in Bronze/Silver")
            summary_table.add_row("Lambda Functions", "‚úÖ Available", "Alert functions ready")
            
            console.print(summary_table)
            
            console.print("\n" + Panel.fit(
                "[bold green]üéâ Phase 2 Data Flow Test Complete![/bold green]\n"
                "[green]‚úÖ Events generated and sent to Kafka[/green]\n"
                "[green]‚úÖ ECS processors consuming and processing[/green]\n"
                "[green]‚úÖ Data being stored in S3 data lake[/green]\n"
                "[green]‚úÖ Lambda functions available for alerts[/green]\n\n"
                "[yellow]Ready for Phase 3: Customer Dashboards & Analytics![/yellow]",
                border_style="green"
            ))
            
        except Exception as e:
            console.print(f"[red]‚ùå Test failed: {e}[/red]")
            raise
        
        finally:
            try:
                producer.close()
            except:
                pass

def main():
    """Main test function"""
    tester = Phase2DataFlowTest()
    tester.run_test()

if __name__ == "__main__":
    main()
